---
layout: post
title: "AI and Slime: ChatGPT is not AI…"
---

…or at least it won’t be in 30 years. Which begs two obvious questions: 1\. What is AI? 2\. if ChatGPT isn’t AI, then what is it?


Before I give my answers to either of those, though, I need to talk about slime.

Slime molds are organisms that live by simple rules: if they sense food, they move towards it by pumping liquid in that direction; if they meet another slime mold, they merge; if they get big enough, they reproduce by making spores; they avoid salt and places they’ve already been; and they dry themselves out if there isn’t enough water. Simple rules. They’re fascinating creatures, if a little unsettling to look at.

They also don’t have brains.

Slime molds are single-celled organisms, like an amoeba. You have trillions of cells in your body, each with its own nucleus and other assorted organelles. Slime molds are about as simple as macroscopic life can be; and yet, they’re incredibly complex to the naked eye. It’s a maxim of biology: simple rules lead to complex behavior. Sometimes that complex behavior [is kind of gross](https://commons.wikimedia.org/wiki/File:Schleimpilz_Urwald_Sababurg.jpg). If you’re interested in slime molds, check out zefrank’s video on them [here](https://www.youtube.com/watch?v=k_GTIL7AECQ), or Sebastian Lague’s video on (kind of) modeling their behavior with software [here](https://www.youtube.com/watch?v=X-iSQQgOd1A).

I also need to talk about cellular automata.

Cellular automata are virtual organisms that live by simple rules. In the case of my favorite, Conway’s Game of Life, the rules are as follows: the game consists of a grid of cells, all of which can be alive or dead; a cell’s neighbors are the eight cells surrounding it; living cells with two or three neighbors continue to live; those with more or less living neighbors die; dead cells with three living neighbors return to life. Simple rules; and yet, those simple rules lead to complex behavior. There is a whole bestiary of organisms that tend to develop in the Game of Life, including organisms that infinitely travel across the grid, producing other organisms that create children of their own. They can be splendid to look at, like the breeder, which builds constructs called glider guns as it moves across the grid:

[![A Conway glider gun breeder](../images/conway1.gif "A Conway glider gun breeder")](https://commons.wikimedia.org/wiki/File:Conways_game_of_life_breeder_animation.gif)

Or the puffer train, which leaves a stable field in its wake:

[![A stable puffer train](../images/conway2.gif "A stable puffer train")](https://commons.wikimedia.org/wiki/File:Conways_game_of_life_breeder_animation.gif)

If you’d like to experiment with the Game of Life, try my Python implementation on the web [here]({{ site.baseurl }}/conway), or on your local machine [here](https://github.com/LevBernstein/simpleConwayLife).

Slime molds and the Game of Life are fascinating, yes. I would not classify them as intelligent. Pretty to look at, capable of remarkable feats, but not intelligent.

See where I’m going with this?

That leads me to question 1: what is AI? The problem with trying to answer this question is that our answer has changed significantly over time. These days, there are lots of processes we take for granted that used to be classified as AI. Consider optical character recognition (or OCR), the process of using computers to identify characters (letters, numbers, symbols). When the technology was invented, it was seen as a major win for AI: computers can now read written text. These days, we see it as the thing that lets us scan QR codes with a phone in dim lighting, or enter text in the fields of a scanned PDF in Adobe Reader. OCR stopped being an innovation in AI, and started being a simple tool. For another example, remember how excited you were when you first got to use Siri? How lifelike it was, what an amazing leap it was for computers that try to be human? I can’t remember the last time I used Siri, and I certainly wouldn’t classify it as AI. The blob detection used in green screens and augmented reality isn’t AI, at least anymore. The late [Larry Tesler](https://en.wikipedia.org/wiki/Larry_Tesler) put it best when he said, “AI is whatever hasn’t been done yet.” The fantastical becomes the mundane remarkably quickly.

This phenomenon, the creatively-named [AI effect](https://en.wikipedia.org/wiki/AI_effect), is really not unique to AI. A car ride stops being a fun journey and starts being a mode of transportation once everyone gets a car; a technology stops being AI once it becomes widely adopted. Still, it’s a particularly sticky issue in AI due to how nebulous the field remains. It’s hard for humans to agree on definitions for anything, much less a nuanced concept like intelligence or sentience. We’ve long moved past the point where the Turing test is sufficient for determining what AI is. At this point, it’s a question for philosophers and logicians.

So, for a working answer to question 1, what is AI? Let’s go with Tesler’s answer, AI is whatever hasn’t be done yet. Even if it was said partially in jest, and even if AI researchers despise the AI effect, I find it useful for understanding how we, the users, think about AI. Moving forward with that definition, then, I can move on to question 2: if ChatGPT isn’t AI, what is it?

I would argue that it’s like a slime mold, or the Game of Life. Machine learning algorithms, for all their complexity and all the labels and buzzwords attached to them (deep learning seems to be the one most in vogue at the moment), work using simple rules. Look under the hood, and you’ll see an algorithm that is rewarded for approaching one state, and discouraged from reaching another. It’s less “learning” how to do a task than it is “learning” how to achieve that task’s success criteria. One could argue the two are the same; that’s immaterial, really. What is important-at least to me-is that it isn’t thought, it isn’t intelligence. It’s complex behavior resulting from simple rules.

This begs the question, then: how are we any different? Don’t humans also generally work off of simple rules? Aren’t our complex behaviors driven by simple biological urges? What is thought?

All of that is above my pay grade.


---

[1](https://commons.wikimedia.org/wiki/File:Schleimpilz_Urwald_Sababurg.jpg)\. Slime mold. Lebrac, [CC BY-SA 3.0 License](https://creativecommons.org/licenses/by-sa/3.0), via Wikimedia Commons

[2](https://www.youtube.com/watch?v=k_GTIL7AECQ)\. "True Facts: The Smartest Slime." Zefrank, YouTube

[3](https://www.youtube.com/watch?v=X-iSQQgOd1A)\. "Coding Adventure: Ant and Slime Simulations." Sebastian Lague, YouTube

[4](https://commons.wikimedia.org/wiki/File:Conways_game_of_life_breeder_animation.gif)\. Breeder animation. Derivative work: George, Conways_game_of_life_breeder.png: Hyperdeath, [CC BY-SA 3.0 License](https://creativecommons.org/licenses/by-sa/3.0), via Wikimedia Commons

[5](https://commons.wikimedia.org/wiki/File:Stable_puffer_animation.gif)\. Puffer animation. Simpsons contributor, Public domain, via Wikimedia Commons